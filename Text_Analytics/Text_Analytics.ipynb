{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jiwanhwang/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['Our research focuses on a new method for assessing and measuring organizationl culture.', 'We used big data processing to mine the ubiquitous digital traces of culture in electronic communications.', 'By studying the language employees use in these communications, we can measure how culture actually influences thier thoughts and behavioral at work.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "text_sample = 'Our research focuses on a new method for assessing and measuring organizationl culture. \\\n",
    "We used big data processing to mine the ubiquitous digital traces of culture in electronic communications. \\\n",
    "By studying the language employees use in these communications, we can measure how culture actually influences thier thoughts and behavioral at work.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)\n",
    "# text_sample is from Harvad Business Review Jan-Feb 2020. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "### separate with space, '.', ',', and etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 14\n",
      "['Our', 'research', 'focuses', 'on', 'a', 'new', 'method', 'for', 'assessing', 'and', 'measuring', 'organizationl', 'culture', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"Our research focuses on a new method for assessing and measuring organizationl culture.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined (sent + word tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['Our', 'research', 'focuses', 'on', 'a', 'new', 'method', 'for', 'assessing', 'and', 'measuring', 'organizationl', 'culture', '.'], ['We', 'used', 'big', 'data', 'processing', 'to', 'mine', 'the', 'ubiquitous', 'digital', 'traces', 'of', 'culture', 'in', 'electronic', 'communications', '.'], ['By', 'studying', 'the', 'language', 'employees', 'use', 'in', 'these', 'communications', ',', 'we', 'can', 'measure', 'how', 'culture', 'actually', 'influences', 'thier', 'thoughts', 'and', 'behavioral', 'at', 'work', '.']]\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jiwanhwang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words (English): 179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
     ]
    }
   ],
   "source": [
    "print('Number of stop words (English):', len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['research', 'focuses', 'new', 'method', 'assessing', 'measuring', 'organizationl', 'culture', '.'], ['used', 'big', 'data', 'processing', 'mine', 'ubiquitous', 'digital', 'traces', 'culture', 'electronic', 'communications', '.'], ['studying', 'language', 'employees', 'use', 'communications', ',', 'measure', 'culture', 'actually', 'influences', 'thier', 'thoughts', 'behavioral', 'work', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "\n",
    "for sentence in word_tokens:\n",
    "    \n",
    "    filtered_words=[]\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    \n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming & Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "happy happiest\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jiwanhwang/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "happy happy\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
    "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
    "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVertorizer and TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COO (Coordinate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice\n",
    "import numpy as np\n",
    "\n",
    "dense = np.array([[3,0,1], [0,2,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "data = np.array([3, 1, 2]) # non-zero data\n",
    "\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS (Compressed Sparse Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "\n",
    "row_pos = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col_pos = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "row_pos_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "## CSR\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"COO\")\n",
    "print(sparse_coo.toarray())\n",
    "print(\"CSR\")\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *** practical use ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "                   [1,4,0,3,2,5],\n",
    "                   [0,6,0,3,0,0],\n",
    "                   [2,0,0,0,0,0],\n",
    "                   [0,0,0,7,0,8],\n",
    "                   [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t1\n",
      "  (0, 5)\t5\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t4\n",
      "  (1, 3)\t3\n",
      "  (1, 4)\t2\n",
      "  (1, 5)\t5\n",
      "  (2, 1)\t6\n",
      "  (2, 3)\t3\n",
      "  (3, 0)\t2\n",
      "  (4, 3)\t7\n",
      "  (4, 5)\t8\n",
      "  (5, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(csr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
